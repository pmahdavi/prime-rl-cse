max_steps = 500
batch_size = 1024
micro_batch_size = 1  # Increased from 1 for better GPU utilization
seq_len = 8192
rollouts_per_example = 8
mask_truncated_completions = false

[model]
name = "willcb/DeepSeek-R1-Distill-Qwen-1.5B"

[sampling]
max_tokens = 8192

[wandb]

[environment]
id = "intellect-math"

[environment.args]
solve_rate_field = "solve_rate_qwen_r1_distill_7b"
min_solve_rate = 0.4
max_solve_rate = 0.9

[wandb.log_extras]
interval = 50

[eval]
interval = 500
eval_base_model = false  # Skip initial evaluation at step 0
environment_ids = ["math500", "aime2025"]
rollouts_per_example = [1, 8]

[ckpt]